# -*- coding: utf-8 -*-
"""HW1-Intel Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UzE20JidXh55k1-bj_f0X9DtWoCkSlh7
"""

import torch
import torchvision
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

from google.colab import drive
drive.mount('/content/drive')

"""We upload our dataset from Google drive and then we unzip the file."""

root_dir = "/content/drive/MyDrive/Intel-Image-Classification.zip"
drive.mount("/content/drive", force_remount=True)

!unzip /content/drive/MyDrive/Intel-Image-Classification.zip -d /content

import os
train_dir = "/content/Intel-Image-Classification/train"
test_dir = "/content/Intel-Image-Classification/test"

from torchvision import transforms as T
from torchvision.datasets import ImageFolder

"""The *torchvision.transforms* module provides various functionality to preprocess the images, here first we resize the image for (100*100) shape, we rotate the images by angle, then transforms them into tensors and finally we apply normalization."""

#Compose transformations

transforms = T.Compose([
        T.Resize((64,64)),
        T.RandomRotation(degrees=(0,20)),
        T.ToTensor(),
        T.Normalize(0.5, 0.5)
    ])

"""We used ImageFolder to load images which is useful in our case. In particularly, our dataset consist of 6 types of images and they are stored in corresponding folders from which the classes are taken."""

train_set = ImageFolder(train_dir, transform=transforms)
test_set = ImageFolder(test_dir, transform=transforms)

data, label = train_set[0]
print(data.shape)

"""Now we compute an exploration of our data. In particularly, in the following codes we will see the length of our train and test sets, the classes and also the number of images per classes in each set."""

# Dataset len
num_train = len(train_set)
num_test = len(test_set)
print(f"Num. training samples: {num_train}")
print(f"Num. test samples:     {num_test}")

print("Following classes are there : \n", train_set.classes)

import pandas as pd
nums = {}
for outcome in train_set.classes:
    nums[outcome] = len(os.listdir(train_dir + '/' + outcome))

img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=["Number of images in the train set"])
img_per_class

numt = {}
for outcome in test_set.classes:
    numt[outcome] = len(os.listdir(test_dir + '/' + outcome))

img_per_classt = pd.DataFrame(numt.values(), index=numt.keys(), columns=["Number of images in the test set"])
img_per_classt

"""In the following code, using the previous results, we can see that the two sets are proportioned."""

numm = {}
for outcome in test_set.classes:
    numm[outcome] = len(os.listdir(test_dir + '/' + outcome))/len(os.listdir(train_dir + '/' + outcome))

img_per_classm = pd.DataFrame(numm.values(), index=numm.keys(), columns=["Number of images in the test set"])
img_per_classm

"""We show an example of image (choosing position 72) in the train set. Since our dataset consists of images in form of Tensors, we use imshow() method of matplotlib python library to visualize images."""

def display_img(img,label):
    print(f"Label : {train_set.classes[label]}")
    plt.imshow(img.permute(1,2,0))

#display the image in the dataset at position 72
display_img(*train_set[72])

"""Now, we proceed randomly splitting the images of the train set in order to get the validation set, choosing as percentage the 10%."""

#Validation set
val_frac= 0.1
num_val = int(num_train * val_frac)
num_train = num_train - num_val

"""We can visualize the new length of the train set and the one of the validation set."""

from torch.utils.data import random_split
 
torch.manual_seed(20)
train_data, val_data = random_split(train_set,[num_train, num_val])
print(f"Length of Train Data : {len(train_data)}")
print(f"Length of Validation Data : {len(val_data)}")

"""We split the dataset into batches choosing as size 64, using *DataLoader()*. 
Then, we show an example of a single batch, using make_grid() which gives us an overall view of images in batch in the form of an image grid.
"""

from torch.utils.data.dataloader import DataLoader

#load the train and validation into batches.
batch_size = 64
train_loader = DataLoader(train_data, batch_size, shuffle = True, drop_last=True, num_workers = 2)
val_loader = DataLoader(val_data, batch_size, shuffle=False, drop_last=False, num_workers = 2)
test_loader = DataLoader(test_set, batch_size, shuffle=False, drop_last=False, num_workers=2)

from torchvision.utils import make_grid
import matplotlib.pyplot as plt

def show_batch(dl):
    """Plot images grid of single batch"""
    for images, labels in dl:
        fig,ax = plt.subplots(figsize = (16,12))
        ax.set_xticks([])
        ax.set_yticks([])
        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))
        break
        
show_batch(train_loader)

#inputs, labels = next(iter(train_loader))
#print(inputs.shape)
#print(labels.shape)

"""We start our model with 5 convolutional layers and then we proceed removing one layer at a time in order to establish which is the one with the best perfomance.

First of all, we create a simple model without FC, and then we see the output dimension of the convolutional layers:
"""

import torch.nn as nn

#SimpleCNN5

class SimpleCNN5(nn.Module):

  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        #Layer 4
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(512),       
        nn.MaxPool2d(kernel_size=2, stride=2),

        #Layer 5
        nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(1024),       
        nn.MaxPool2d(kernel_size=2, stride=2)

    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x)
    return x

"""Our 2D CNN model expects as input a tensor of shape: [N, C, H, W], where:

*   N is the number of samples (batch_size);
*   C is the number of channels (1 for grayscale images, 3 for RGB images);
*   H and W are height and width dimension of the images;

We have to add the batch dimension to img_5.
"""

#Create the model
model = SimpleCNN5()
img_5, label_5 = train_data[0]
img_5 = img_5.unsqueeze(dim=0)
#Get the output
output = model(img_5)
out_features = output.size(1) * output.size(2) * output.size(3)
print(out_features)

"""Now we can add some fully connected layers to our CNN model and the obtained result 4096 is the number of features of input for the first FC."""

#CNN5

class CNN5(nn.Module):
  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        #Layer 4
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(512),       
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        #Layer 5
        nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(1024),       
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

    # Create fully-connected layers
    self.fc_layers = nn.Sequential(
        # a first FC layer
        nn.Linear(4096, 1024),
        nn.Dropout(p=0.25),
        nn.ReLU(),
        # a second FC layer
        nn.Linear(1024, 512),
        nn.Dropout(p=0.25),
        nn.ReLU(),       
        # the final Classification Layer
        nn.Linear(512,6)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x) # before passing to fc layer, we have to flatten x
    x = x.view(x.size(0), -1) 
    output = self.fc_layers(x) 
    return output

# Create the model
model = CNN5()
output = model(img_5)

torch.cuda.is_available()

dev = torch.device('cuda')
print(dev)

"""We define an optimizer using the Stochastic Gradient Descent and setting a learning rate= 0.01. 
Then we also define the loss criterion using the Cross Entropy Loss.
"""

# Define an optimizier
import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr = 0.01)
# Define a loss 
criterion = nn.CrossEntropyLoss()

def train(net, loaders, optimizer, criterion, epochs=100, dev=torch.device('cpu')):
    try:
        net = net.to(dev)
        print(net)
        # Initialize history
        history_loss = {"train": [], "val": [], "test": []}
        history_accuracy = {"train": [], "val": [], "test": []}
        results= [0,0,0]
        val_loss = None
        # Process each epoch
        for epoch in range(epochs):
            # Initialize epoch variables
            sum_loss = {"train": 0, "val": 0, "test": 0}
            sum_accuracy = {"train": 0, "val": 0, "test": 0}
            # Process each split
            for split in ["train", "val", "test"]:
                if split == "train":
                  net.train()
                else:
                  net.eval()
                # Process each batch
                for (input, labels) in tqdm(loaders[split]):
                    # Move to CUDA
                    input = input.to(dev)
                    labels = labels.to(dev)
                    # Reset gradients
                    optimizer.zero_grad()
                    # Compute output
                    pred = net(input)
                    loss = criterion(pred, labels)
                    # Update loss
                    sum_loss[split] += loss.item()
                    # Check parameter update
                    if split == "train":
                        # Compute gradients
                        loss.backward()
                        # Optimize
                        optimizer.step()
                    # Compute accuracy
                    _,pred_labels = pred.max(1)
                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)
                    # Update accuracy
                    sum_accuracy[split] += batch_accuracy
            # Compute epoch loss/accuracy
            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in ["train", "val", "test"]}
            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in ["train", "val", "test"]}
            # Update history
            for split in ["train", "val", "test"]:
                history_loss[split].append(epoch_loss[split])
                history_accuracy[split].append(epoch_accuracy[split])
            # Print info
            print(f"Epoch {epoch+1}:",
                  f"TrL={epoch_loss['train']:.4f},",
                  f"TrA={epoch_accuracy['train']:.4f},",
                  f"VL={epoch_loss['val']:.4f},",
                  f"VA={epoch_accuracy['val']:.4f},",
                  f"TeL={epoch_loss['test']:.4f},",
                  f"TeA={epoch_accuracy['test']:.4f},")
            if val_loss == None or epoch_loss['val'] < val_loss:
                val_loss = epoch_loss['val']
                results = [epoch_accuracy['train'], epoch_accuracy['val'], epoch_accuracy['test']]
    except KeyboardInterrupt:
        print("Interrupted")
    finally:
        # Plot loss
        plt.title("Loss")
        for split in ["train", "val", "test"]:
            plt.plot(history_loss[split], label=split)
        plt.legend()
        plt.show()
        # Plot accuracy
        plt.title("Accuracy")
        for split in ["train", "val", "test"]:
            plt.plot(history_accuracy[split], label=split)
        plt.legend()
        plt.show()
        return results

# Define dictionary of loaders
loaders = {"train": train_loader,
           "val": val_loader,   
           "test": test_loader}

"""Now we train the model for 10 epochs."""

# Train model
result5 = train(model, loaders, optimizer, criterion, epochs=10, dev= dev)

"""We repeat the procedure removing one layer at a time in our model."""

#SimpleCNN4

class SimpleCNN4(nn.Module):

  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        #Layer 4
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(512),       
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x)
    return x

#Create the model
model = SimpleCNN4()
img_4, label_4 = train_data[0]
img_4 = img_4.unsqueeze(dim=0)
#Get the output
output = model(img_4)
out_features = output.size(1) * output.size(2) * output.size(3)
print(out_features)

#CNN4

class CNN4(nn.Module):
  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        #Layer 4
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(512),       
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

    # Create fully-connected layers
    self.fc_layers = nn.Sequential(
        # a first FC layer
        nn.Linear(18432, 4096),
        nn.Dropout(p=0.25),
        nn.ReLU(),
        # a second FC layer
        nn.Linear(4096, 1024),
        nn.Dropout(p=0.25),
        nn.ReLU(),       
        # the final Classification Layer
        nn.Linear(1024,6)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x) # before passing to fc layer, we have to flatten x
    x = x.view(x.size(0), -1) 
    output = self.fc_layers(x) 
    return output

# Create the model
model4 = CNN4()
output = model4(img_4)
output.shape

# Define an optimizier
optimizer = optim.SGD(model4.parameters(), lr = 0.01)
# Define a loss 
#criterion = nn.CrossEntropyLoss()

# Train model_4layers
result4= train(model4, loaders, optimizer, criterion, epochs=10, dev= dev)

#SimpleCNN3

class SimpleCNN3(nn.Module):

  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x)
    return x

#Create the model
model = SimpleCNN3()
img_3, label_3 = train_data[0]
img_3 = img_3.unsqueeze(dim=0)
#Get the output
output = model(img_3)
out_features = output.size(1) * output.size(2) * output.size(3)
print(out_features)

#CNN3

class CNN3(nn.Module):
  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2),
        
        # Layer 3
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(256),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

    # Create fully-connected layers
    self.fc_layers = nn.Sequential(
        # a first FC layer
        nn.Linear(50176, 4096),
        nn.Dropout(p=0.25),
        nn.ReLU(),
        # a second FC layer
        nn.Linear(4096, 1024),
        nn.Dropout(p=0.25),
        nn.ReLU(),       
        # the final Classification Layer
        nn.Linear(1024,6)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x) # before passing to fc layer, we have to flatten x
    x = x.view(x.size(0), -1) 
    output = self.fc_layers(x) 
    return output

# Create the model
model = CNN3()
output = model(img_3)

# Define an optimizier

optimizer = optim.SGD(model.parameters(), lr = 0.01)
# Define a loss 
#criterion = nn.CrossEntropyLoss()

# Train model_3layers
result3= train(model, loaders, optimizer, criterion, epochs=10, dev= dev)

#SimpleCNN2

class SimpleCNN2(nn.Module):

  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x)
    return x

#Create the model
model = SimpleCNN2()
img_2, label_2 = train_data[0]
img_2 = img_2.unsqueeze(dim=0)
#Get the output
output = model(img_2)
out_features = output.size(1) * output.size(2) * output.size(3)
print(out_features)

#CNN2

class CNN2(nn.Module):
  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        
        #Layer 2
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1),
        nn.ReLU(),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

    # Create fully-connected layers
    self.fc_layers = nn.Sequential(
        # a first FC layer
        nn.Linear(115200, 4096),
        nn.Dropout(p=0.25),
        nn.ReLU(),
        # a second FC layer
        nn.Linear(4096, 1024),
        nn.Dropout(p=0.25),
        nn.ReLU(),       
        # the final Classification Layer
        nn.Linear(1024,6)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x) # before passing to fc layer, we have to flatten x
    x = x.view(x.size(0), -1) 
    output = self.fc_layers(x) 
    return output

# Create the model
model = CNN2()
output = model(img_2)

# Define an optimizier

optimizer = optim.SGD(model.parameters(), lr = 0.01)
# Define a loss 
#criterion = nn.CrossEntropyLoss()

# Train model_2layers
result2 = train(model, loaders, optimizer, criterion, epochs=10, dev= dev)

#SimpleCNN1

class SimpleCNN1(nn.Module):

  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x)
    return x

#Create the model
model = SimpleCNN1()
img_1, label_1 = train_data[0]
img_1 = img_1.unsqueeze(dim=0)
#Get the output
output = model(img_1)

out_features = output.size(1) * output.size(2) * output.size(3)
print(out_features)

#CNN1

class CNN1(nn.Module):
  #Constructor
  def __init__(self):
    # Call parent contructor
    super().__init__()
    self.conv_layer = nn.Sequential(
        # Layer 1
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=0),
        nn.ReLU(),
        nn.BatchNorm2d(64),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

    # Create fully-connected layers
    self.fc_layers = nn.Sequential(
        # a first FC layer
        nn.Linear(61504, 4096),
        nn.Dropout(p=0.25),
        nn.ReLU(),
        # a second FC layer
        nn.Linear(4096, 1024),
        nn.Dropout(p=0.25),
        nn.ReLU(),       
        # the final Classification Layer
        nn.Linear(1024,6)
    )

  # Forward
  def forward(self, x):
    x = self.conv_layer(x) # before passing to fc layer, we have to flatten x
    x = x.view(x.size(0), -1) 
    output = self.fc_layers(x) 
    return output

# Create the model
model = CNN1()
output = model(img_1)

# Define an optimizier

optimizer = optim.SGD(model.parameters(), lr = 0.01)
# Define a loss 
#criterion = nn.CrossEntropyLoss()

# Train model_1layers
result1 = train(model, loaders, optimizer, criterion, epochs=10, dev= dev)

percent5 = [str(round(num *100,2)) + '%' for num in result5]
percent4 = [str(round(num *100,2)) + '%' for num in result4]
percent3 = [str(round(num *100,2)) + '%' for num in result3]
percent2 = [str(round(num *100,2)) + '%' for num in result2]
percent1 = [str(round(num *100,2)) + '%' for num in result1]

"""Here we show a summary table to easily read the result of each model and estabilish which is the best one. 
In our case we obtain that the Convolutional Neural Network with four layers (CNN4) has the best performance.
"""

import pandas as pd

results=[percent5,percent4,percent3,percent2, percent1]
models= ['5 Layers', '4 Layers', '3 Layers', '2 Layers', '1 Layer']

df=pd.DataFrame(results, columns=['Train', 'Validation', 'Test'], index= models, dtype= str)
df

"""We also show the confusion matrix which compares the actual target values with those predicted by the model."""

#CONFUSION MATRIX
#function that perform inference
def predict(net, loader,dev):
  net.to(dev)
  predictions_list = []
  with torch.no_grad():
    for (input, labels) in loader:
      # Move to CUDA
      input = input.to(dev)
      pred = net(input)
      _,pred_labels = pred.max(1)
      pred_labels.tolist()
      for lab in pred_labels:
        lab = lab.item()
        predictions_list.append(str(lab))
  return predictions_list

#inference on test set
predicted_labels = predict(model4, test_loader, dev=dev)

#extracting ground truth labels
true_labels = []
for _ ,label in test_loader:
  for lab in label:
    lab = lab.item()
    true_labels.append(str(lab))

labels = [str(x) for x in range(0,6)]
print (labels)

#confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_true=true_labels, y_pred=predicted_labels, labels=labels)
cm

import seaborn as sns
import numpy as np

cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=train_set.classes, yticklabels=train_set.classes)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)